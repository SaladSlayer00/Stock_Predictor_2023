# -*- coding: utf-8 -*-
"""LSTM_Draft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EgcWAkHZUjPBUOpEQHRqpKVFB_-kITdc
"""

!pip install pyspark

!pip install pyarrow

!pip install yahoo_fin

!pip install lime

from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import VectorAssembler
from yahoo_fin import stock_info as si
import datetime as dt
from pyspark.ml import Pipeline
import pandas
import pandas_datareader as pdr
from pyspark.sql.functions import year, month, dayofmonth, col
import matplotlib.pyplot as plt
from lime.lime_tabular import LimeTabularExplainer
from pyspark.sql.functions import last, first, coalesce
from pyspark.sql.functions import col, max as max_, min as min_
from pyspark.sql.functions import year, month, dayofmonth, col
from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
from pyspark.sql.functions import col
from pyspark.sql.functions import monotonically_increasing_id
import pyspark.pandas as pd
import pyspark.sql.functions as F
from pyspark.sql.window import Window

import os
import sys

os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

"""# Data Collection"""

#Setting parameters for data collection

ticker = 'TSLA'
start_date = dt.datetime(2010, 1, 1)
end_date = dt.datetime(2023, 1, 1)
fred_symbols = ['UNRATE', 'GDP', 'FEDFUNDS', 'CPIAUCNS', 'M2', 'DGS10', 'PCE', 'T10Y2Y', 'USROA', 'USROE', 'WTISPLC', 'HOUST', 'INDPRO', 'PAYEMS', 'BAMLH0A0HYM2', 'GS10', 'BASE', 'RIFSPPFAAD01NB', 'EXUSEU', 'NETEXP']

#Calling APIs to gather information

historical_data = si.get_data(ticker, start_date, end_date, interval='1d')
fred_df = pdr.get_data_fred(fred_symbols, start_date, end_date)

"""# Data Pre-Processing with Spark"""

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("StockPrediction") \
    .config("spark.sql.debug.maxToStringFields", "100") \
    .getOrCreate()

#Dataframe building
historical_data_spark = spark.createDataFrame(historical_data.reset_index())
fred_df_spark = spark.createDataFrame(fred_df.reset_index())

print(historical_data_spark)

#Performing operations on dataframe through SparkSQL methods

historical_data_spark = historical_data_spark.withColumnRenamed("index", "DATE")
historical_data_spark = historical_data_spark.drop('ticker', 'adjclose')

historical_data_spark = historical_data_spark.withColumn("year", year("DATE"))
historical_data_spark = historical_data_spark.withColumn("month", month("DATE"))
historical_data_spark = historical_data_spark.withColumn("day", dayofmonth("DATE"))

historical_data_spark.show()

fred_df_spark = fred_df_spark.withColumn("Index", monotonically_increasing_id())
#fred_df_spark.show()

#Performing joining of the two dataframes on DATE, ordering and removing column

dataset_spark = historical_data_spark.join(fred_df_spark, on="DATE", how="left")
dataset_spark = dataset_spark.orderBy("DATE")
dataset_spark = dataset_spark.withColumn("Index", monotonically_increasing_id())
dataset_spark = dataset_spark.drop("DATE")
dataset_spark.show()

# Assuming 'df' is your Spark DataFrame
row_count = dataset_spark.count()

# Print the row count
print("Number of rows in the DataFrame: ", row_count)

'''
def stringReplaceFunc(x, y):
    return F.when(x != y, x).otherwise(F.lit(None))

def forwardFillImputer(df, cols=[], partitioner="Index", value='NaN'):
    for c in cols:
        df = df.withColumn(c, F.when(F.col(c) != value, F.col(c)))
        df = df.withColumn(c, F.coalesce(F.col(c), F.last(c, True).over(Window.orderBy(partitioner)), F.lit('0')))
    return df
'''

#Filling NaN values with the last non-null number

from pyspark.sql.window import Window
import pyspark.sql.functions as F

def forwardFillImputer(df, cols=[], partitioner="Index", value='NaN'):
    for c in cols:
        # Define the window specification with the partitioner
        window_spec = Window.orderBy(partitioner)

        # Replace value with NULL
        df = df.withColumn(c, F.when(F.col(c) != value, F.col(c)).otherwise(F.lit(None)))

        # Forward fill using the last non-null value within the partition
        df = df.withColumn(c, F.last(c, True).over(window_spec))

    return df

dataset_spark = forwardFillImputer(dataset_spark, cols=[i for i in fred_symbols])
dataset_spark.show()

# Drop rows where 'column1' or 'column2' have null values
dataset_spark = dataset_spark.dropna()
row_count = dataset_spark.count()

# Print the row count
print("Number of rows in the DataFrame: ", row_count)

"""# LSTM Attempt"""

df_plot = dataset_spark.select('Index', 'close').toPandas()
df_plot.set_index('Index', inplace=True)
df_plot.plot(figsize=(16, 6), grid=True)
plt.title('Tesla stock')
plt.ylabel('Stock Quote ($)')
plt.show()

dataset_spark.toPandas().shape

"""# MLLib"""

# Prepare data for MLlib
feature_columns = [col_name for col_name in dataset_spark.columns if col_name != 'close' and col_name != 'Index']
vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
# Determine the split point based on the desired ratio
split_ratio = 0.9  # 80% for training, 20% for testing
split_point = int(dataset_spark.count() * split_ratio)

# Split the data into training and testing sets
train_data = dataset_spark.limit(split_point)
test_data = dataset_spark.subtract(train_data)

print(feature_columns)

train_data_plot = train_data.select('Index', 'close').toPandas()
train_data_plot.set_index('Index', inplace=True)
train_data_plot.plot(figsize=(16, 6), grid=True)
plt.title('Tesla Stock Train Interval')
plt.ylabel('Stock Quote ($)')
plt.show()

test_data_plot = test_data.select('Index', 'close').toPandas()
test_data_plot.set_index('Index', inplace=True)
test_data_plot.plot(figsize=(16, 6), grid=True)
plt.title('Tesla Stock Test')
plt.ylabel('Stock Quote ($)')
plt.show()

import numpy as np
trainArray = np.array(train_data.select('*').collect())
testArray = np.array(test_data.select('*').collect())

print(trainArray[0])
print('-------------')
print(testArray[0])

from sklearn.preprocessing import MinMaxScaler, StandardScaler

'''
# Combine training and test data
combined_data = np.vstack((trainArray, testArray))

# Initialize a MinMaxScaler
minMaxScale = MinMaxScaler()

# Fit the scaler on the combined data
minMaxScale.fit(combined_data)

# Transform both training and test data
trainArray_scaled = minMaxScale.transform(trainArray)
testArray_scaled = minMaxScale.transform(testArray) '''

minMaxScale = MinMaxScaler()
minMaxScale.fit(trainArray)
testingArray = minMaxScale.transform(testArray)
trainingArray = minMaxScale.transform(trainArray)

print(testingArray[0])
print('--------------')
print(trainingArray[0])

print("Min:", trainingArray.min())
print("Max:", trainingArray.max())
print("Min:", trainingArray.min())
print("Max:", trainingArray.max())

'''xtrain = trainArray_scaled[:, 0:-1]
xtest = testArray_scaled[:, 0:-1]
# ytrain = trainingArray[:, 5]
# ytest = testingArray[:, 5]
ytrain = trainArray_scaled[:, -1:]
ytest = testArray_scaled[:, -1:]'''

#Extracting training and testing set
num_features = trainingArray.shape[1]

x_start = 0
x_end = 2
xtrain = trainingArray[:, x_start:x_end]

x_start = x_end
x_end = 3
xtrain = np.concatenate((xtrain, trainingArray[:, x_start:x_end]), axis=1)

x_start = 4
x_end = num_features
xtrain = np.concatenate((xtrain, trainingArray[:, x_start:x_end]), axis=1)


ytrain = trainingArray[:, 3:4]


# Same for test data
x_start = 0
x_end = 2
xtest = testingArray[:, x_start:x_end]

x_start = x_end
x_end = 3
xtest = np.concatenate((xtest, testingArray[:, x_start:x_end]), axis=1)

x_start = 4
x_end = num_features
xtest = np.concatenate((xtest, testingArray[:, x_start:x_end]), axis=1)

ytest = testingArray[:, 3:4]

trainingArray[0]

xtrain[0]

ytrain[0]

print('xtrain shape = {}'.format(xtrain.shape))
print('xtest shape = {}'.format(xtest.shape))
print('ytrain shape = {}'.format(ytrain.shape))
print('ytest shape = {}'.format(ytest.shape))

plt.figure(figsize=(16,6))
plt.plot(xtrain[:,0],color='red', label='open')
plt.plot(xtrain[:,1],color='blue', label='high')
plt.plot(xtrain[:,2],color='green', label='low')
#plt.plot(xtrain[:,3],color='purple', label='close')
plt.legend(loc = 'upper left')
plt.title('Featires')
plt.xlabel('Index')
plt.ylabel('Scaled Quotes')
plt.show()

plt.figure(figsize=(16,6))
plt.plot(xtrain[:,3],color='black', label='volume')
plt.legend(loc = 'upper right')
plt.title('Volume by Day')
plt.xlabel('Index')
plt.ylabel('Scaled Volume')
plt.show()

"""# Keras

"""

from keras import models, layers
import tensorflow as tf

input_shape = (1, 28)

model = models.Sequential()
model.add(layers.LSTM(1, input_shape=input_shape))
model.add(layers.Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

model.summary()

xtrain = xtrain.reshape((2827, 1, 28))
xtest  = xtest.reshape((315,1,28))

print('The shape of xtrain is {}: '.format(xtrain.shape))
print('The shape of xtest is {}: '.format(xtest.shape))

print('The shape of xtrain is {}: '.format(xtrain.shape))
print('The shape of xtest is {}: '.format(xtest.shape))

loss = model.fit(xtrain, ytrain, batch_size=10, epochs=50)

plt.plot(loss.history['loss'], label = 'loss')
plt.title('mean squared error by epoch')
plt.legend()
plt.show()

predicted = model.predict(xtest)

combined_array = np.concatenate((ytest, predicted), axis = 1)

plt.figure(figsize=(16,6))
plt.plot(combined_array[:,0],color='red', label='actual')
plt.plot(combined_array[:,1],color='blue', label='predicted')
plt.legend(loc = 'lower right')
plt.title('Actual vs. Predicted TESLA Stock')
plt.xlabel('Index')
plt.ylabel('Scaled Quotes')
plt.show()

import sklearn.metrics as metrics
np.sqrt(metrics.mean_squared_error(ytest,predicted))

